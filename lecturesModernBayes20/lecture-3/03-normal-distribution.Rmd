
---
title: "Module 3: Introduction to the Normal Distribution"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Announcements
===

- Thank you to everyone that completed the survey. 
- I'll provide some actionable items below. 

Feedback
===

- The github repo has been cleaned up extensively. 
- Exams will be solely online (during class period) by feedback from class, and will be open note and open book. 
- "Please move the lab time." These weren't set by me, and I don't have the authority to do this. 
- "Please go faster/slower." "Great pace." 
- "I'd like more OH hours." The TA's and I are pretty flexible if you have a question or need to meet. Can anyone not make any OH due to a class conflict? If so, see me after class. 
- The class is quite divided regarding being online and being in person. We'll do out best to work this out in a safe way for everyone.
- "Lab is not helping me." 

Resources 
===

Don't let the resources be overwhelming. 

Instead, figure out what works the best for you and use them. What works for one student may be drastically different than what works for another student. 

My goal is to set you up for success and provide you with many different ways to succeed so you're not struggling. 

If you find yourself struggling, please let me know so I can help you. 


Agenda
===
- The normal distribution
- The variance versus precision
- The re-parameterized normal distribution 
- Common properties
- The normal-uniform model
- The normal-normal model

Normal distribution
===

The normal distribution $\N(\mu,\sigma^2)$

- with mean $\mu\in\R$ and variance $\sigma^2 > 0$ - (standard deviation $\sigma =\sqrt{\sigma^2}$) has p.d.f.
$$\N(x\mid\mu,\sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}(x-\mu)^2\Big) $$
for $x\in\R$. 

It is often more convenient to write the p.d.f.\ in terms of the **precision**, or inverse variance, $\lambda = 1/\sigma^2$ rather than the variance. 

Re-parameterized Normal
===
In this parametrization, the p.d.f.\ is
$$\N(x\mid\mu,\lambda^{-1}) =\sqrt{\frac{\lambda}{2\pi}}\exp\big(-\tfrac{1}{2}\lambda (x-\mu)^2\big) $$
since $\sigma^2 = 1/\lambda =\lambda^{-1}$.


\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/normal.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Normal distribution with various choices of $\mu$ and $\sigma$.}
\end{figure}

Normality?
===

- The central limit theorem (CLT) states that the sum of a large number of independent random variables tends to be approximately normally distributed.  
- Real world data often appears approximately normal. 

Normality?
===

- Human heights and other body measurements, 
- Cumulative hydrologic measures such as annual rainfall or monthly river discharge, 
- Errors in astronomical or physical observations, 
- Diffusion of a substance in a liquid or gas. 

- Some things are products of many independent variables (rather than sums), and in such cases the logarithm will be approximately normal since it is a sum of many independent variables

Example: stock market indices, due to the effect of compound interest.

Properties of the Normal distribution
===
\begin{itemize}
\item Mean, median, and mode are all the same ($\mu$)
\item Symmetric about the mean
\item 95\% probability within $\pm 1.96\sigma$ of the mean (roughly, $\pm 2\sigma$)
\item If $X\sim\N(\mu,\sigma^2)$ and $Y\sim\N(m,s^2)$ independently, then
\begin{align}\label{equation:linear-combination}
a X + b Y\sim\N(a\mu + b m,\,a^2\sigma^2+ b^2 s^2).
\end{align}
\item Careful: \texttt{rnorm}, \texttt{dnorm}, \texttt{pnorm}, and \texttt{qnorm} in \texttt{R} take the mean and \textcolor{red}{standard deviation} $\sigma$ as arguments (not mean and variance $\sigma^2$). For example, \texttt{rnorm(n,m,s)} generates $n$ normal random variables from $\N(m,s^2)$.
\end{itemize}





Normal-Uniform
===
$$ X_1,\dotsc,X_n \mid \theta \iid\N(\theta,\sigma^2). $$

Assume the prior on $\theta$ is constant over the real line. We can write this as $p(\theta) \propto 1,$ where $-\infty < \theta < \infty.$

Derive the posterior distribution.\footnote{Please observe that the posterior is not conjugate to the prior in this situation, so it's very important to make sure that the posterior is a proper distribution.}

Solution
===

\begin{align}
p(\theta \mid x_{1:n}) 
&\propto
\textcolor{blue}{\N(x_{1:n} \mid \theta,\sigma^2)} \times 1 \\
& \propto (\frac{\ell}{2\pi})^{n/2}\exp\big(-\tfrac{1}{2}\ell \sum_i(x_i-\theta)^2\big)\notag\\
& \propto \exp\big(-\tfrac{1}{2}\ell \sum_i(x_i -\bar{x} + \bar{x} - \theta)^2\big)\notag\\
& \propto \exp\big(-\tfrac{1}{2}\ell \sum_i(x_i -\bar{x})^2)
\exp\big(-\tfrac{1}{2}\ell \sum_i( \bar{x} - \theta)^2\big)\notag\\
&\propto 
\exp\big(-\tfrac{1}{2}\ell \sum_i( \bar{x} - \theta)^2\big) \\
&= \textcolor{red}{\exp\big(-\tfrac{n \ell}{2} (\theta - \bar{x} )^2\big)}
= \textcolor{blue}{\N(\theta \mid \bar{x}, (n\ell)^{-1}).}
\end{align}

This implies that 
$$\theta \mid x_{1:n} \sim N(\bar{x}, (n\ell)^{-1}) = N(\bar{x}, \sigma^2/n)$$

Commonly asked questions on the Normal-Uniform derivation
===

1. Why do we add and subtract $\bar{x}$? 

This trick makes the cross product term 1. 


2. Why is the cross product term 1? The cross product term is: 

\begin{align*}
&\exp\big(-\tfrac{2}{2}\ell)\sum_i(x_i -\bar{x})(\bar{x} - \theta)\\
&=\exp\big(-\ell)(\bar{x} - \theta)\sum_i(x_i -\bar{x})\\
&=e^{0}=1.
\end{align*}

Normal-Normal
===
$$ X_1,\dotsc,X_n \mid \theta \iid\N(\theta,\lambda^{-1}). $$
Assume the precision $\lambda = 1/\sigma^2$ is known and fixed, and $\theta$ is given a $\N(\mu_0,\lambda_0^{-1})$ prior:
$$\btheta \sim \N(\mu_0,\lambda_0^{-1})$$
i.e., $p(\theta) = \N(\theta\mid \mu_0,\lambda_0^{-1})$. This is sometimes referred to as a **Normal--Normal** model.

Posterior derivation
===
We begin with the **likelihood** of the normal distribution. \textcolor{blue}{We work with proportionality with respect to $\theta$ as we will combine the **likelihood** and **prior** to find the posterior, where the parameter of interest is $\theta.$}

\textcolor{red}{For any $x$ and $\ell$},
\begin{align}\label{equation:prop}
\N(x\mid\theta,\ell^{-1})
& =\sqrt{\frac{\ell}{2\pi}}\exp\big(-\tfrac{1}{2}\ell (x-\theta)^2\big)\notag\\
&\underset{\theta}{\propto} \exp\big(-\tfrac{1}{2}\ell (x^2 - 2 x \theta +\theta^2)\big)\notag\\
&\textcolor{red}{\underset{\theta}{\propto} \exp\big(\ell x \theta -\tfrac{1}{2}\ell\theta^2)\big)}.
\end{align}

Note: we drop the **constant term** and we will do this often when working with the normal distribution. 


Posterior derivation (continued)
===
We now consider the **prior** distribution on $\theta.$

Due to the symmetry of the normal p.d.f.,
\begin{align}\label{equation:prior}
\N(\theta\mid\mu_0,\lambda_0^{-1}) =
\sqrt{\frac{\lambda_o}{2\pi}}\exp\big(-\tfrac{1}{2}\lambda_o (\theta - \mu_o)^2\big)\notag\\
= \sqrt{\frac{\lambda_o}{2\pi}}\exp\big(-\tfrac{1}{2}\lambda_o (\mu_o - \theta)^2\big)\notag\\
= \N(\mu_0\mid\theta,\lambda_0^{-1})
\underset{\theta}{\propto}\textcolor{red}{\exp\big(\lambda_0\mu_0\theta-\tfrac{1}{2}\lambda_0\theta^2\big)},
\end{align}
where $x=\mu_0$ and $\ell=\lambda_0$.

Posterior derivation (continued)
===
Let $$L =\lambda_0+ n\lambda \quad \text{and} \quad
M =\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda}.$$
\begin{align*}
p(\theta|x_{1:n})&\propto p(\theta) p(x_{1:n}|\theta) \\
&= \N(\theta\mid\mu_0,\lambda_0^{-1})\prod_{i = 1}^n\N(x_i\mid \theta,\lambda^{-1})\\
&\overset{\text{(a)}}{\propto} \exp\big(\lambda_0\mu_0\theta-\tfrac{1}{2}\lambda_0\theta^2\big)
         \exp\big(\lambda (\textstyle\sum x_i) \theta -\tfrac{1}{2}n\lambda\theta^2\big)\\
&= \exp\Big((\lambda_0\mu_0+\lambda\textstyle\sum x_i)\theta-\tfrac{1}{2}(\lambda_0+ n\lambda)\theta^2\Big)\\
&= \exp(L M\theta-\tfrac{1}{2}L\theta^2)\\
&\overset{\text{(b)}}{\propto} \N(M\mid\theta,L^{-1}) =\N(\theta\mid M,L^{-1}),
\end{align*}
where step (a) uses Equations \ref{equation:prop} and \ref{equation:prior}, and step (b) uses Equation \ref{equation:prop} with $x=M$ and $\ell=L$.


Posterior derivation (continued)
===
Recall $$L =\lambda_0+ n\lambda \quad \text{and} \quad
M =\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda}.$$

It turns out that the posterior is

\begin{equation}
\label{equation:posterior}
\textcolor{red}{\btheta|x_{1:n}\, \sim \,\N(M,L^{-1})}
\end{equation}


i.e., $p(\theta|x_{1:n}) =\N(\theta\mid M,L^{-1})$.


Thus, the normal distribution is, itself, a conjugate prior for the mean of a normal distribution with known precision.

Heights of Adult Humans
===
- Heights tend to be normally distributed because there are many independent genetic and environmental factors which contribute additively to overall height
- This leads to a normal distribution due to the central limit theorem. 
\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-female-male.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Estimated densities of the heights of Dutch women and Dutch men based on a sample of 695 women and 562 men.}
  \label{figure:heights}
\end{figure}


Heights of Adult Humans
===
- Consider combined distribution of heights (pooling females and males together). Would this be normal? 
- It is thought that such data is bimodal (having two maxima). Is it really bimodal? (See, Schilling et al.\ (2002))
\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-combined.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Estimated density for Dutch women and men together, assuming there is an equal proportion of women and men in the population.}
  \label{figure:heights-combined}
\end{figure}



Heights of Adult Humans, Combined
===
At a glance, while the heights of women and men separately do appear to be roughly normally distributed, the combined distribution does not look bimodal.  How could we test whether it is bimodal in a more precise way?

Our Assumptions
===
- Assume female heights and male heights are each normally distributed. 
- \textcolor{red}{Assume both female heights and male heights have different means but the same standard deviation.}
- \textcolor{red}{Assume that there is an equal proportion of women and men in the population.} 
- Then, it is known that the combined distribution is bimodal if and only if the difference between the means is greater than twice the standard deviation (Helguerro, 1904).

Model
===
In mathematical notation: Assume the female heights are
$$X_1,\dotsc,X_k\iid\N(\theta_f,\sigma^2),$$
where $k=695$, the male heights are
$$Y_1,\dotsc,Y_\ell\iid\N(\theta_m,\sigma^2),$$
where $\ell=562$, and the p.d.f.\ of the combined distribution of heights is
$$\tfrac{1}{2}\N(x\mid\theta_f,\sigma^2)+\tfrac{1}{2}\N(x\mid\theta_m,\sigma^2). $$
(This is an example of what is called a two-component \term{mixture} distribution.) 

Model
===
Let's put independent normal priors on $\theta_f$ and $\theta_m$:
$$ p(\theta_f,\theta_m) = p(\theta_f) p(\theta_m) 
=\N(\theta_f\mid \mu_{0,f},\sigma_0^2)\N(\theta_m\mid \mu_{0,m},\sigma_0^2).$$

- Assume $\sigma^2$ is known. 
- For the purposes of this example, let's use $\sigma=8$ centimeters (about 3 inches). 
- Based on common knowledge of typical human heights, let's choose the prior parameters (a.k.a. hyperparameters) as follows:
\begin{center}
\begin{tabular}{cll}
$\mu_{0,f}$ & (mean of prior on female mean ht) & \text{165 cm ($\approx$ 5 ft, 5 in)}\\
$\mu_{0,m}$ & (mean of prior on male mean ht) & \text{178 cm ($\approx$ 5 ft, 10 in)}\\
$\sigma_0$ & (std.\ dev.\ of priors on mean ht) & \text{15 cm ($\approx$ 6 in)}\\
\end{tabular}
\end{center}

Bimodal Fact
===
It is known (Helguerro, 1904) that the combined distribution is bimodal if and only if
$$ |\theta_f-\theta_m|>2\sigma. $$
So, to address our question of interest (``Is human height bimodal?''), we would like to compute the posterior probability that this is the case, i.e., we want to know
$$ \Pr(\text{bimodal}\mid \text{data}) = \Pr\big(|\btheta_f-\btheta_m|>2\sigma \mid x_{1:k},y_{1:\ell}\big).$$ 

Results
===
\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-prior-posterior.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Priors and posteriors for the mean heights of Dutch women and men.}
  \label{figure:heights-prior-posterior}
\end{figure}

Results (continued)
===
We can compute the posteriors for $\theta_f$ and $\theta_m$ using \textcolor{red}{Equation \ref{equation:posterior}} for each of them, independently.  Figure \ref{figure:heights-prior-posterior} shows the priors and posteriors. 
\begin{itemize}
\item Sample means: $\bar x = 168.0$ cm (5 feet 6.1 inches) for females, and $\bar y = 181.4$ cm (5 feet 11.4 inches) for males.
\item Posterior means: $M_f = 168.0$ cm for females, and $M_m = 181.4$ cm for males. (Essentially identical to the sample means, due to the relatively large sample size and relatively weak prior.)
\item Posterior standard deviations: $1/\sqrt{L_f} = 0.30$ cm and $1/\sqrt{L_m} = 0.34$ cm.
\end{itemize}



Results (continued)
===
By Equation \ref{equation:linear-combination} (a linear combination of independent normals is normal),
$$\btheta_m-\btheta_f\mid x_{1:k},y_{1:\ell}\,\sim\,\N(M_m-M_f,\, L_m^{-1} + L_f^{-1}) = \N(13.4,0.45^2) $$
so we can compute $\Pr(\text{bimodal}\mid \text{data})$ using the normal c.d.f.\ $\Phi$:
\begin{align*}
&\Pr(\text{bimodal}\mid \text{data})= \Pr\big(|\btheta_m-\btheta_f|>2\sigma \mid x_{1:k},y_{1:\ell}\big) \\
&= \Phi(-2\sigma\mid 13.4,0.45^2)
+ \big(1-\Phi(2\sigma\mid 13.4,0.45^2)\big)\\
& = 6.1\times 10^{-9}.
\end{align*}


Intuitive interpretation: The posteriors are about 13 or 14 centimeters apart, which is under the $2\sigma = 16$ threshold for bimodality, and they are sufficiently concentrated that the posterior probability of bimodality is essentially zero.

Takeaways
===

- We reviewed the univariate normal distribution and properties of it (such as symmetry about the mean). 
- We discussed types of data that empirically have a normal distribution to motivate its usage. 
- We dervied the Normal-Uniform model, where the uniform distribution is the first improper prior we have seen in this course. 
- We derived the Normal-Normal model. 
- We considered an application of human heights from Schilling et al. (2002), where we showed that the posterior probability of bimodality was zero (regarding a plot of combined heights). 

Detailed Takeaways for Exam
===

- Working with the univariate normal distribution
- Knowing the precision and variance relationship
- Knowing the shape of the normal distribution
- Knowing the CLT
- Knowing what type of data is approximately normal and what data is not
- Knowing properties of the normal that we will often work with such as symmetry about the mean 
- Being able to derive the posterior of the normal-uniform model
- Understanding that a non-informative prior is highly informative and why. 
- Being able to derive the normal-normal posterior distribution
- Knowing how to write out the posterior mean and variance intuitively

Detailed Takeaways for Exam (Continued)
===

- You should be able to work with a two-component mixture model (as in the case study)
- Suppose I provide you with a case study such as the one on human heights. You should be able to explain why it makes sense to model the data as normal and what paramaters would be reasonable given the description. 
- Given as case study, you should be able to incorporate a pilot (or prior study) into your prior distribution and back up how you incorporate it. 
- You should be able to update the posterior accordingly. 
- You be able to state benefits of your model specification and weaknesses. 
- You should be able to state a better model that would be more realistic once we have covered the normal-normal-gamma model (module 4). 
- You should be able to discuss sensitity of the posterior analysis for any hyper-parameters. 



Exercise
===

Recall that Recall $$L =\lambda_0+ n\lambda \quad \text{and} \quad
M =\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda}.$$

What happens to the posterior mean and the precision as $n \rightarrow \infty.$



Exercise
====

Let's consider the posterior mean. 
$$
\begin{aligned}
M &=\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda} \\
&=\frac{\lambda_0\mu_0+\lambda n \bar{x}}{\lambda_0+ n\lambda} \\
&=\frac{\lambda_0\mu_0/n+\lambda  \bar{x}}{\lambda_0/n+ \lambda} \\
\end{aligned}
$$

As $n \rightarrow \infty,$

$$M \rightarrow \lambda  \bar{x}/\lambda = \bar{x}.$$

Exercise
===

Let's consider the posterior variance. 

$$L^{-1} =\frac{1}{\lambda_0+ n\lambda}$$ 

As $n \rightarrow \infty,$

$$L^{-1} = \frac{1}{\lambda_0+ n\lambda} = \frac{1/n}{\lambda_0/n+ \lambda} \approx \frac{1}{n \lambda} \rightarrow 0 $$ 

Interpretation
===

What can we learn from this example? 

If our sample size is large enough (for any application or real world example), then 

\begin{enumerate}
\item the posterior mean will tend to the the sample mean.
\item the posterior variance will tend to 0. 
\end{enumerate}

Module 3 Class Notes
===

Module 3 Class Notes can be found here:

https://github.com/resteorts/modern-bayes/tree/master/lecturesModernBayes20/lecture-3/03-class-notes

