
---
title: "Module 4: Introduction to the Normal Gamma Model"
author: "Rebecca C. Steorts and Lei Qian"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---


Agenda
===
- Continue with more conjugate distributions
- Define the precision 
- Introduce the Normal-Gamma model
- Consider our first hiearchical model with more than two levels
- Deriving the posterior for the Normal likelihood and Normal-Gamma prior
- Application to IQ scores (spurters versus controls)


NormalGamma distribution
===
Let

$X_1,\dotsc,X_n\mid \mu,\lambda \iid\N(\mu,\lambda^{-1})$ and assume **both**

- the mean $\mu$ and 
- the precision $\lambda= 1/\sigma^2$ are **unknown**. 

The $\NormalGamma(m,c,a,b)$ distribution, with $m\in\R$ and $c,a,b>0$, is a joint distribution on $(\mu,\lambda)$ obtained by letting
\begin{align*}
\bm\mu|\lambda \,\,&\sim \N(m,(c\lambda)^{-1})\\
\bm\lambda \,\,&\sim\Ga(a,b)
\end{align*}

In other words, the joint p.d.f. is
$$ p(\mu,\lambda) = p(\mu|\lambda) p(\lambda) =\N(\mu\mid m,(c\lambda)^{-1})\Ga(\lambda\mid a,b) $$
which we will denote by $\NormalGamma(\mu,\lambda\mid m,c,a,b).$


NormalGamma distribution (continued)
===
It turns out that this provides a **conjugate prior** for $(\mu,\lambda)$.

One can show the posterior is
\begin{align}\label{equation:NormalGamma-posterior}
\bm\mu,\bm\lambda|x_{1:n}\,\sim\,\NormalGamma(M,C,A,B)
\end{align}
i.e., $p(\mu,\lambda|x_{1:n}) =\NormalGamma(\mu,\lambda\mid M,C,A,B)$, where
\begin{align*}
    M & =\frac{c m +\sum_{i=1}^n x_i}{c + n}\\
    C & = c + n\\
    A & = a + n/2\\
    B & = b +\tfrac{1}{2}\big(c m^2-C M^2+\textstyle\sum_{i=1}^n x_i^2\big).
\end{align*}

NormalGamma distribution (continued)
===
For interpretation, $B$ can also be written (by rearranging terms) as 
\begin{align}\label{equation:B}
B = b + \frac{1}{2}\sum_{i=1}^n (x_i-\bar x)^2 + \frac{1}{2}\frac{c n}{c + n}(\bar x - m)^2.
\end{align}

In this module, we will derive the posterior derivation of this hierarchical model, which has three levels (or layers) to it.\footnote{The data is considered the first level, the prior on $\lambda$ is considered the second level, and the prior on $\mu$ is considered the third level of the hierarchical model.}

NormalGamma distribution (continued)
===
\begin{itemize}
\item $M$: Posterior mean for $\mu$. It is a weighted average (convex combination) of the prior mean and the sample mean:
$$ M =\frac{c}{c + n} m + \frac{n}{c + n}\bar x. $$
\item $C$: ``Sample size'' for estimating $\mu$. (The standard deviation of $\mu|\lambda$ is $\lambda^{-1/2}/\sqrt C$.)
\item $A$: Shape for posterior on $\lambda$. Grows linearly with sample size.
\item $B$: Rate (1/scale) for posterior on $\lambda$. Equation \ref{equation:B} decomposes $B$ into the prior variation, observed variation (sample variance), and variation between the prior mean and sample mean:
\end{itemize}
\begin{align}
 B &= (\text{prior variation}) + \tfrac{1}{2}n\text{(observed variation)}\\ &+ \tfrac{1}{2}\tfrac{c n}{c + n}\text{(variation bw means)}. 
 \end{align}

Posterior derivation 
===
First, consider the NormalGamma density. Dropping constants of proportionality, multiplying out $(\mu-m)^2 =\mu^2 - 2\mu m + m^2$, and collecting terms, we have
\begin{align}
&\NormalGamma(\mu,\lambda\mid m,c,a,b) \\
&= \N(\mu\mid m,(c\lambda)^{-1})\Ga(\lambda\mid a,b)\notag\\
&= \sqrt{\frac{c\lambda}{2\pi}}\exp\Big(-\tfrac{1}{2} c\lambda(\mu-m)^2\Big)
\frac{b^a}{\Gamma(a)}\lambda^{a-1}\exp(-b\lambda)\notag\\
&\underset{\mu,\lambda}{\propto} 
\textcolor{red}{\lambda^{a-1/2}\exp\Big(-\tfrac{1}{2}\lambda(c\mu^2 - 2 c m\mu + c m^2 + 2 b)\Big)}.\label{equation:NormalGamma}
\end{align}

Posterior derivation (continued)
===
Similarly, for any $x$,
\begin{align}
\N(x\mid\mu,\lambda^{-1}) & =\sqrt{\frac{\lambda}{2\pi}}\exp\Big(-\tfrac{1}{2}\lambda(x-\mu)^2\Big)\notag\\
&\underset{\mu,\lambda}{\propto} \textcolor{red}{\lambda^{1/2}\exp\Big(-\tfrac{1}{2}\lambda(\mu^2 - 2 x \mu + x^2)}\Big).
\label{equation:individual}
\end{align}

Posterior derivation (continued)
===
\begin{small}
\begin{align*}
&p(\mu,\lambda|x_{1:n})\\ &\underset{\mu,\lambda}{\propto} p(\mu,\lambda) p(x_{1:n}|\mu,\lambda) \\
&\underset{\mu,\lambda}{\propto}
\textcolor{red}{\lambda^{a-1/2} \exp\Big(-\tfrac{1}{2}\lambda(c\mu^2 - 2 c m\mu + c m^2 + 2 b)\Big)}\\
  & \qquad\times \textcolor{red}{\lambda^{n/2} \exp\Big(-\tfrac{1}{2}\lambda(n\mu^2 - 2 (\textstyle\sum x_i) \mu + \sum x_i^2)\Big)}\\
& = \lambda^{a+n/2-1/2}
\exp\Big(-\tfrac{1}{2}\lambda\big((c+n)\mu^2 - 2 (c m +\textstyle\sum x_i) \mu + \textcolor{blue}{c m^2 + 2 b + \sum x_i^2}\big)\Big)\\
&\overset{\text{(a)}}{=}\lambda^{A-1/2}\exp\Big(-\tfrac{1}{2}\lambda\big(C\mu^2 - 2 C M \mu + \textcolor{blue}{C M^2 + 2 B}\big)\Big)\\
&\overset{\text{(b)}}{\propto} \NormalGamma(\mu,\lambda\mid M,C,A,B)
\end{align*}
where step (b) is by Equation \ref{equation:NormalGamma},
and step (a) holds if $A=a+n/2$, $\,C=c+n$, $\,C M = (c m + \textstyle\sum x_i)$, and
$$C M^2 + 2 B = c m^2 + 2 b + \sum x_i^2.$$
\end{small}
Posterior derivation (continued)
===
This choice of $A$ and $C$ match the claimed form of the posterior, and 
solving for $M$ and $B$, we get $M = (c m + \sum x_i)/(c+n)$ and 
$$ B = b +\tfrac{1}{2}(c m^2-C M^2+\textstyle\sum x_i^2), $$
as claimed.

Alternative derivation: complete the square all way (longer and much more tedious).

Do a teacher's expectations influence student achievement?
===
Do a teacher's expectations influence student achievement?
In a famous study, Rosenthal and Jacobson (1968) performed an experiment in a California elementary school to try to answer this question. At the beginning of the year, all students were given an IQ test.  For each class, the researchers randomly selected around 20\% of the students, and told the teacher that these students were ``spurters'' that could be expected to perform particularly well that year. (This was not based on the test---the spurters were randomly chosen.) At the end of the year, all students were given another IQ test. 
The change in IQ score for the first-grade students was:\footnote{The original data is not available. This data is from the \texttt{ex1321} dataset of the \texttt{R} package \texttt{Sleuth3}, which was constructed to match the summary statistics and conclusions of the original study.}

**This applied example corresponds to lab 4 and homework 5.**

Do a teacher's expectations influence student achievement?
===
```{r, echo=FALSE}
library(plyr)
library(ggplot2)
library(dplyr)
library(xtable)
library(reshape)
```

Spurters/Control Data
===
```{r}
#spurters
x <- c(18, 40, 15, 17, 20, 44, 38)
#control
y <- c(-4, 0, -19, 24, 19, 10, 5, 10,
      29, 13, -9, -8, 20, -1, 12, 21,
      -7, 14, 13, 20, 11, 16, 15, 27,
      23, 36, -33, 34, 13, 11, -19, 21,
      6, 25, 30,22, -28, 15, 26, -1, -2,
      43, 23, 22, 25, 16, 10, 29)
iqData <- data.frame(Treatment = 
      c(rep("Spurters", length(x)),
      rep("Controls", length(y))),
      Gain = c(x, y))
```

An initial exploratory analysis
===
Plot the **number of students** versus the **change in IQ score** for the two groups. How strongly does this data support the hypothesis that the teachers' expectations caused the
spurters to perform better than their classmates? 

Histogram of Change in IQ Scores
===
```{r}
xLimits = seq(min(iqData$Gain) - (min(iqData$Gain) %% 5),
              max(iqData$Gain) + (max(iqData$Gain) %% 5),
              by = 5)

ggplot(data = iqData, aes(x = Gain, 
                          fill = Treatment, 
                          colour = I("black"))) + 
  geom_histogram(position = "dodge", alpha = 0.5, 
                 breaks = xLimits, closed = "left")+
  scale_x_continuous(breaks = xLimits, 
                     expand = c(0,0))+ 
  scale_y_continuous(expand = c(0,0), 
                     breaks = seq(0, 10, by = 1))+
  ggtitle("Histogram of Change in IQ Scores") + 
  labs(x = "Change in IQ Score", fill = "Group") + 
  theme(plot.title = element_text(hjust = 0.5))  


```

Histogram of Change in IQ Scores
===
```{r, echo=FALSE}
xLimits = seq(min(iqData$Gain) - (min(iqData$Gain) %% 5),
              max(iqData$Gain) + (max(iqData$Gain) %% 5),
              by = 5)

ggplot(data = iqData, aes(x = Gain, 
                          fill = Treatment, 
                          colour = I("black"))) + 
  geom_histogram(position = "dodge", alpha = 0.5, 
                 breaks = xLimits, closed = "left")+
  scale_x_continuous(breaks = xLimits, 
                     expand = c(0,0))+ 
  scale_y_continuous(expand = c(0,0), 
                     breaks = seq(0, 10, by = 1))+
  ggtitle("Histogram of Change in IQ Scores") + 
  labs(x = "Change in IQ Score", fill = "Group") + 
  theme(plot.title = element_text(hjust = 0.5))  


```

IQ Tests and Modeling
===
IQ tests are purposefully calibrated to make the scores normally distributed, so it makes sense to use a normal model here:
\begin{align*}
\text{spurters: } X_1,\dotsc,X_{n_S}\iid \N(\mu_S,\lambda_S^{-1})\\
\text{controls: } Y_1,\dotsc,Y_{n_C}\iid \N(\mu_C,\lambda_C^{-1}).
\end{align*}

- We are interested in the difference between the means---in particular, is $\mu_S>\mu_C$?

- We don't know the standard deviations $\sigma_S=\lambda_S^{-1/2}$ and $\sigma_C=\lambda_C^{-1/2}$, and the sample seems too small to estimate them very well.


IQ Tests and Modeling
===
On the other hand, it is easy using a Bayesian approach: we just need to compute the posterior probability that $\mu_S>\mu_C$:
$$ \Pr(\bm\mu_S > \bm\mu_C \mid x_{1:n_S},y_{1:n_C}). $$
Let's use independent NormalGamma priors:
\begin{align*}
\text{spurters: } (\bm\mu_S,\bm\lambda_S) \sim \NormalGamma(m,c,a,b)\\
\text{controls: } (\bm\mu_C,\bm\lambda_C) \sim \NormalGamma(m,c,a,b)
\end{align*}

Hyperparameter settings
===
\begin{itemize}
\item $m = 0$ (Don't know whether students will improve or not, on average.)
\item $c = 1$ (Unsure about how big the mean change will be---prior certainty in our choice of $m$ assessed to be equivalent to one datapoint.)
\item $a = 1/2$ (Unsure about how big the standard deviation of the changes will be.)
\item $b = 10^2 a$ (Standard deviation of the changes expected to be around $10 = \sqrt{b/a} = \E(\lambda)^{-1/2}$.)
\end{itemize}



```{r,echo=FALSE}
prior = data.frame(m = 0, c = 1, a = 0.5, b = 50)
findParam = function(prior, data){
  postParam = NULL
  c = prior$c
  m = prior$m
  a = prior$a
  b = prior$b
  n = length(data)
  postParam = data.frame(m = (c*m + n*mean(data))/(c + n), 
                c = c + n, 
                a = a + n/2, 
                b =  b + 0.5*(sum((data - mean(data))^2)) + 
                  (n*c *(mean(data)- m)^2)/(2*(c+n)))
  return(postParam)
}
postS = findParam(prior, x)
postC = findParam(prior, y)

sim = 10000

lambdas = rgamma(sim, postS$a, postS$b)
lambdac = rgamma(sim, postC$a, postS$b)
mus = sapply(sqrt(1/(postS$c*lambdas)),rnorm, n = 1, mean = postS$m)
muc = sapply(sqrt(1/(postC$c*lambdac)),rnorm, n = 1, mean = postC$m)


simDF = data.frame(lambda = c(lambdas, lambdac),
                   mu = c(mus, muc),
                   Treatment = rep(c("Spurters", "Controls"),
                                   each = sim))
simDF$lambda = simDF$lambda^{-0.5}

sim = 10000

lambda = apply(as.matrix(sim), 1,rgamma, prior$a, prior$b)
mu = sapply(sqrt(1/(prior$c*lambda)), rnorm, n = 1, mean = prior$m)

simPrior = data.frame(lambda, mu)
simPrior$lambda = simPrior$lambda^{-0.5}
```

Prior Samples
===
```{r,echo=FALSE}
ggplot(data = simPrior, aes(x = mu, y = lambda)) +
  geom_point(alpha = 0.5, colour = "darkgreen") + 
  labs(x = expression(paste(mu, " (Mean Change in IQ Score)")),
       y = expression(paste(lambda^{-1/2}, " (Std. Dev. of Change)"))) +
  ggtitle("Prior Samples")+ 
  theme(plot.title = element_text(hjust = 0.5))+ 
  xlim(-50,50) + ylim(0, 40)
```


Original question
===
"What is the posterior probability that $\mu_S>\mu_C$?"
 
- The easiest way to do this is to take a bunch of samples from each of the posteriors, and see what fraction of times we have $\mu_S>\mu_C$. 

- This is an example of a Monte Carlo approximation (much more to come on this in the future). 

- To do this, we draw $N=10^6$ samples from each posterior:

\begin{align*}
&(\mu_S^{(1)},\lambda_S^{(1)}),\dotsc,(\mu_S^{(N)},\lambda_S^{(N)})\sim \NormalGamma(A_1,B_1,C_1,D_1)\\
&(\mu_C^{(1)},\lambda_C^{(1)}),\dotsc,(\mu_C^{(N)},\lambda_C^{(N)})\sim\NormalGamma(A_2,B_2,C_2,D_2)
\end{align*}

Original question (continued)
===
What are the updated posterior values? 

and obtain the approximation
\begin{align*}
\Pr(\bm\mu_S > \bm\mu_C \mid x_{1:n_S},y_{1:n_C}) 
\approx \frac{1}{N} \sum_{i = 1}^N I\big(\mu_S^{(i)}>\mu_C^{(i)}\big) = ??.
\end{align*}

**You will explore this more in lab 4 and homework 5.**

Takeaways
===

- We introduced the Normal-Gamma distribution.
- We defined the precision (inverse of the variance). 
- We defined a hiearchical model with three levels. 
- We derived the posterior for the Normal likelihood and the Normal-Gamma prior. What did this involve? 
- We utilized our modeling techniques this week on an application to IQ scores. How realistic was this model in practice? 
- If you made changes to the model, what changes would you make and what complications might you run into? 

Detailed Takeaways
===

- In order to understand this module, you must be able to derive the normal-normal model. 
- You must have a solid foundation regarding the normal distribution and its properties. 
- You must know the relationship between the precision and the variance. 
- You should be able to derive the normal-normal-gamma model. 
- You should conceptually understand why this model is important and when it's used for a case study. 
- You should understand conceptually what each of the four parameters of the normal-gamma distribution represent. 
- The case study for this problem (on IQ scores) is a good review problem regarding when you should use the normal-normal-gamma model over the normal-normal model. This is something that you should be able to conceptually walk through for this case study or a similar case study. 


Module 4 Notes
===

Module 4 Class Notes can be found here:

https://github.com/resteorts/modern-bayes/tree/master/lecturesModernBayes20/lecture-4/04-class-notes